{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0efc5590-e8f2-40ee-8c6f-b462ccc581e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/cdsw/.local/lib/python3.7/site-packages (3.3.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/cdsw/.local/lib/python3.7/site-packages (from pyspark) (0.10.9.5)\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (pyspark.py, line 16)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3457\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_214/2593607172.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from pyspark.sql import SparkSession\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/cdsw/sessions/pyspark.py\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    !pip3 install pyspark\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "  .appName('cml-training-pyspark') \\\n",
    "  .getOrCreate()\n",
    "\n",
    "# Now you can use the `SparkSession` named `spark` to read\n",
    "# data into Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "376d2e15-4ba5-4a22-bcca-9b50e770dbe0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_204/4103451580.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# automatically from the data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mflights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/flights.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# ## Reading Data\n",
    "\n",
    "# Read the flights dataset. This data is in CSV format\n",
    "# and includes a header row. Spark can infer the schema\n",
    "# automatically from the data:\n",
    "\n",
    "flights = spark.read.csv('data/flights.csv', header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fac69-25bf-4ea8-97ef-ac5ff7a64cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Using SQL Queries\n",
    "\n",
    "# Instead of using Spark DataFrame methods, you can\n",
    "# use a SQL query to achieve the same result.\n",
    "\n",
    "# First you must create a temporary view with the\n",
    "# DataFrame you want to query:\n",
    "\n",
    "flights.createOrReplaceTempView('flights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156970e-3674-4ed0-a3e9-03539ad0e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then you can use SQL to query the DataFrame:\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  SELECT origin,\n",
    "    COUNT(*) AS num_departures,\n",
    "    AVG(dep_delay) AS avg_dep_delay\n",
    "  FROM flights\n",
    "  WHERE dest = 'SFO'\n",
    "  GROUP BY origin\n",
    "  ORDER BY avg_dep_delay\"\"\").toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
